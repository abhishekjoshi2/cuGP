<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>cuGP by sgoyal78 and abhishekjoshi2</title>


  <script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
  </script>

 <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML"></script>

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">

    <style>
    table, td, th {    
	border: 1px solid #ddd;
	text-align: left;
    }

    table {
	border-collapse: collapse;
	}

	th, td {
	padding: 15px;
	}

  </style>
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">cuGP</h1>
      <h2 class="project-tagline">GPU Accelerated Gaussian Process for Regression</h2>
      <a href="https://github.com/abhishekjoshi2/cuGP" class="btn">View on GitHub</a>
      <a href="https://github.com/abhishekjoshi2/cuGP/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/abhishekjoshi2/cuGP/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">



<!--
      <h3>
<a id="Checkpoint Update" class="anchor" href="#checkpoint" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Checkpoint Update</h3>


<ul>

<li> <b>Implementing Gaussian Process Regression in C++:</b></li>
<p> As planned, we have successfully implemented Gaussian Process Regression in C++. This proved to be harder than what we had expected, since many of the implementation intricacies cropped up as we started coding each step. We have tested our code against an implementation of Gaussian Process Regression in Matlab (GPML toolbox by Rasmussen and Nickisch), and we have achieved full accuracy in all stages of the computation.</p>

<li> <b>Implementing 2 Covariance Kernels:</b></li>
<p> We have implemented 2 covariance kernels: Squared Exponential Kernel along with the Noise Kernel. For now, we have pushed the task of implementing the third covariance kernel to a lower priority task, since we don't think it is crucial to our task of "parallelizing Gaussian Process", and would be more suited to a primarily ML-focused project. Moreover, implementing SE Kernel + Noise Kernel is sufficient for implementing the whole training and regression task.
</p>

<li> <b>Naively parallelizing Gaussian Process in CUDA:</b></li>
<p> As planned, we have also successfully ported a naive version of Gaussian Process regression code in CUDA. This involved breaking down many of the steps of the computation into modular and reusable parts of code that are amenable to be run on CUDA. We will now briefly describe the different kernels that we have implemented as a part of the whole stack. For all kernels, X is the input training matrix, N is the number of training samples, and DIM is the number of parameters for each training sample:</p>

<ul>
<li> <b> compute_K_train: </b></li>

<p> We launch N*N threads - one for each element in the N*N covariance matrix. Each thread picks up its required input working set from the X matrix and fills up the (i,j)th entry in the K matrix, which is the covariance matrix. This can be further improved by collaborative loading of X into shared memory.</p>

<li> <b> compute_cholesky: </b></li>
<p> We use Cholesky factorization to factorize the K matrix into a lower triangular matrix L (and its corresponding upper triangular matrix U). Cholesky factorization is further broken down into a series of kernels such as taking offsetted transpose, rectangular forward substitution, generic matrix transpose, matrix multiplication, and offsetted matrix subtraction. We implemented parallel Cholesky by referring to the algorithm given in [DA14]. Currently, for taking the pure Cholesky in the first step of the recursive process, we have fixed our <i>b</i> parameter to be 2. Going forward, we hope to try doing the same for <i>b = 3</i> and <i>b = 4</i>.</p>

<li> <b> matrix_transpose: </b></li>
<p> Matrix transpose is needed at several points during training, and therefore we have written a naive implementation of matrix transpose in CUDA, where the (i, j)th thread fills up the (j, i)th entry in the transpose matrix. We are not sure if there are better ways to do this, and we will be on the look out for a better implementation. </p>

<li> <b> matrix_multiplication: </b></li>
<p> Matrix multiplication is another important piece of our implementation, and currently we have a naively parallel version of matrix multiplication where we launch one thread for every element of the output matrix. Going forward, we plan to implemented the same using shared memory and collaboratively loading data sets into shared memory.</p>

<li> <b> rectangular_forward_substitution: </b></li>
<p> Forward substitution helps us solve equations of the form <i><b>AX = B</b></i> where A, X, B are matrices, A and B are known, and X is unknown. For now, we launch one thread for each column of X, since each column is independent of each other.</p>

<li> <b> determinant_lower_triangular_matrix: </b></li>
<p> We need the determinant of the lower triangular matrix U. We thought we might have to implement a parallel version for finding the determinant, but later realized that the determinant can be naively taken by taking the square of the product of the diagonal elements! We launch one thread for taking the determinant, which sequentially walks the whole diagonal and calculates the product of all the elements.</p>

<li> <b> vector_forward_substution and vector_backward_substitution: </b></li>
<p> This is the most sequential part of the whole system, and is currently a major bottleneck. For now, we have an extremely naively parallel version in CUDA, which doesn't perform well. We had expected this, and will now implement a different algorithm for implementing these two kernels, which is more amenable to parallelizing on CUDA.</p>

<li> <b> compute_squared_distance, elementwise_matrix_mult, subtract_matrices, get_outer_product: </b></li>
<p> All these kernels have been naively parallelized by launching one thread for each element, since each of them can be done without any dependencies. </p>

</ul>

<li> <b> Schedule and Progress </b></li>
Please find our progress below (at the end of the page). We would say that we are in sync with our proposed schedule :D.
</ul>

<h3><a id="updatedschedule" class="anchor" href="#updatedschedule" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Updated Schedule</h3>

<ul>

<li>22nd April</li>

<ul>
<li>Improve performance of all kernels except vector forward and backward substitution. Use blocking and shared memory as much as possible.</li>
<li>Start work on performance evaluation to find places where the framework takes a long time. Find bottlenecks. </li>
</ul>

<li>26th April</li>
<ul>
<li>Improve performance of vector forward and backward substitution</li>
</ul>

<li>30th April</li>
<ul> 
<li>Start work on distributed version of cuGP if possible. Otherwise, continue work on forward and backward substitution optimization. (This is exam week, so we don't want to go overboard on our tasks). </li>
</ul>

<li>4th May</li>
<ul>
<li>Finish all work</li>
<li>Evaluate system performance and fine tune different parameters</li>
</ul>

<li>7th May</li>
<ul>
<li>Code drop in</li>
<li>Finish writeup</li>
</ul>
</ul>

<h3><a id="finaldemo" class="anchor" href="#finaldemo" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Final Demo</h3>
For the final demo, we will show graphs of our system's performance and show speedups as against the serial version.

  <h3>
<a id="Prelim-results" class="anchor" href="#presults" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Preliminary Results</h3>

<p>
The figure below shows the time it takes to obtain the Cholesky decomposition for 2 different dataset sizes (N = 500 and N = 1000 samples) for the serial GP implementation vs the naive GPU implemnentation. We see that the speedup for N = 1000 is around 10x.
<img src="images/compare_chol.png" alt="comparison of Cholesky">

The figure below shows the time taken for computing the log marginal likelihood for the 2 implementations. The worse performance of our naive GPU implementation is as expected. This is because our forward subsitution and backward substition modules are run by a single CUDA thread (due to the inherent sequential nature of the algorithm itself). This time includes the time taken by the Cholesky and the forward-backward substitution routines.
<img src="images/compare_ll.png" alt="comparison of log likelihood">
</p>



  <h3>
<a id="Current-Challenges" class="anchor" href="#challenges" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Current Challenges</h3>

We have identified 2 key challenges that have to be addressed for an efficient final CUDA implementation:
<ul>
<li> <b> Cholesky decomposition </b> </li>
<p> In the computation of the log marginal-likelihood we need to perform Cholesky decomposition of the covariance matrix. As of now we have implemented a blocked-recursive algorithm for computing it. However, we think that a better use of shared memory and a hybrid algorithm would surely be beneficial. </p>

<li> <b> Triangular Matrix Inversion </b> </li> 
<p> Triangular matrix inversion (TMI) basically comprises of the forward-substitution routines for both vectors as well as matrices. Due to the inherently sequential algorithm for solving forward/backward substitution for a vector, currently, our CUDA kernel implemention for it is quite naive. We have gone through some recent papers, and have found a recursive algorithm that seems to be more CUDA friendly. So one of our primary goals is to implement an efficient triangular matrix inversion module.   </p>
  
</ul>

<h3>
<a id="nicetohaves" class="anchor" href="#nicetohaves" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Nice-to-have</h3>
We are contemplating moving to a distributed enviroment once we optimize our single node performance. We would then be able to extract extremely high performance from multiple GPUs. However, this is definitely a stretch goal for us, and would be undertaken only after we feel confident about our single node performance.

<hr>
<hr>

<h3><a id="summary" class="anchor" href="#summary" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Summary</h3>

<p>We are going to parallelize the training and prediction phases of Gaussian Process Regression on a GPU.</p>

<h3>
<a id="background" class="anchor" href="#background" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Background</h3>

<p>Gaussian Process (GP) regression models have become very popular in recent years in the machine learning community. This can be attributed to the flexibility offered by these models to allow users to choose the appropriate covariance function to control the degree of smoothness. GP regression is basically a Bayesian nonparametric model (where an underlying finite-dimensional random variable is replaced by a stochastic process). Formally, a GP is a stochastic process where any finite number of random variables have a joint Gaussian distribution [RW06]. </p>

<p>A GP is completely specified by a mean function ($m(\vec{x})$) and a covariance function ($\kappa (\vec{x}_1, \vec{x}_2)$), which for a real process $f(\vec{x})$ can be defined as:</p>

<p>$$ m(\vec{x}) = \mathbf{E}[f(\vec{x})] \ 
\kappa (\vec{x}_1, \vec{x}_2) =  \mathbf{E}[(f(\vec{x}_1) - m(\vec{x}_1))( f(\vec{x}_2) - m(\vec{x}_2)  )] $$</p>

<p>Generally, the mean function is taken as zero. In that case, the covariance function  ($\kappa (\vec{x}_1, \vec{x}_2)$) basically turns into the covariance between $f(\vec{x}_1)$ and $f(\vec{x}_2)$. The squared exponential (SE) kernel is a common choice of covariance function (amongst a wide range of available choices for the kernel), and is defined as follows:
$$ \kappa (\vec{x}_1, \vec{x}_2)  = \exp( -\frac{1}{2l^2} | \vec{x}_1 -  \vec{x}_2 |^2 ),$$
where, $l$ is the lengthscale parameter that needs to be estimated.</p>


<p>In order to perform nonparametric regression, a GP prior is placed over the unknown function. The posterior distribution obtained by multiplying the prior with the likelihood is again a GP. The predictive distribution for a new test point ( $ \vec{x}_{t} $ ), given the entire dataset ($n$ tuples of the form $(\vec{x}, y)$) can be shown to be a Gaussian with the following mean ( $ \hat{f_{t}} $ ) and variance ( $ \mathbf{V}[f_{t}] $ ):
$$ \hat{f_{t}} = {\vec{k}_{t}}^{T} (K + \sigma_{n}^2I)^{-1} \vec{y} $$

$$ \mathbf{V}[f_{t}] = \kappa (\vec{x}_{t}, \vec{x}_{t}) - {\vec{k}_{t}}^{T} (K + \sigma_{n}^2I)^{-1} \vec{k}_{t} $$
where,  </p>

<ul>
<li> $K$ is a $n \times n$ matrix where the $ij$th entry is computed as $K_{ij} = \kappa (\vec{x}_i, \vec{x}_j)$,
<li>$\vec{y}$ is a $n \times 1$ vector formed by stacking the $n$ target values together from the dataset,</li>
<li>$I$ is a $n \times n$ identity matrix,</li>
<li>$\sigma^2$ is the noise variance,</li>
<li>$ \vec{k}_{t}$ is a $n \times 1$ vector where each entry is computed as $ \vec{k}_{t_{i}} = \kappa (\vec{x}_i, \vec{x}_{t})$</li>
</ul>


<p>In order to learn the different parameters (lengthscale(s) of the covariance kernel, the noise and signal variances), a common technique is to use the marginal likelihood maximization framework (this step can be thought of as the 'training' phase). The log marginal likelihood has the following form:</p>

<p>$$ LML = \log p(\vec{y} | X) = -\frac{1}{2} \vec{y}^T (K + \sigma_{n}^2I)^{-1}\vec{y} - \frac{1}{2} \log |K + \sigma_{n}^2I | - \frac{n}{2} \log 2\pi    $$  </p>

<p>If we consider $\vec{\theta}$ as the vector of parameters to estimate, gradient based methods (conjugate gradient of L-BFGS) can be employed after obtaining suitable analytic gradients $ \frac{\partial LML}{ \partial \vec{\theta}_i}$.</p>

<p>The main bottleneck for both training and prediction phases is the matrix inversion step which requires $\mathcal{O}(n^3)$ flops, thus limiting the application of GPs for large datasets having $ n &gt; 10000$ samples.</p>

<p>Our main task in this project is to exploit the data parallelism available while constructing the $K$ matrix, as well as efficiently computing the matrix inverse. If time permits we would like to look at recent approximate approaches that might be more suitable for distributed GPU setting. [DN15, WN15]</p>

<h3>
<a id="challenges" class="anchor" href="#challenges" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Challenges</h3>

<ul>
<li>Understand Gaussian Processes and then design an efficient implementation for performing parallel Regression on the same.</li>
<li>Gaussian Processes rely heavily on matrix operations, such as matrix multiplication matrix inversion. Thus, our main challenge will be to ensure that these bandwidth-bound operations are done efficiently on a GPU.</li>
<li>We hope to leverage currently available matrix operation routines provided by Magma [reference]. However, the Magma routines are built for general purpose matrix operations, and may have to sacrifice on application-specific optimizations. Therefore, another challenge for us is to try and exploit data access patterns and data locality to squeeze out performance while using general-purpose libraries.</li>
<li>We understand that achieving linear speedup as the number of cores on the GPU increases may be difficult to achieve due to the inherent nature of the algorithm to update some global state during each phase of the training phase. Therefore, much of our focus will be on achieving maximum acceleration on each individual iteration of the training phase.</li>
<li>Another challenge will be to ensure that the system scales as the size of the input and the processing power increase.
Now-a-days, the main focus is not on raw speedup obtained, but speedup obtained per dollar spent. Therefore, another challenge will be to explore whether it is truly better to implement Gaussian Processes on a GPU, or are we better off on a CPU.</li>
<li>The limitations of resources such as shared memory will limit the scaling that we may able to achieve as the problem size grows. However, given the resources that we have, we should be able to achieve good speedups for workloads upto a certain point beyond which the resources will make it tough to keep going at the same level of performance.</li>
</ul>

<h3>
<a id="plan-to-achieve" class="anchor" href="#plan-to-achieve" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Plan to achieve</h3>

<ul>
<li>Implement a correct, parallel, and efficient implementation of Gaussian Processes on a wide variety of data sets.</li>
<li>Implement at least 3 covariance kernels.</li>
<li>Implement code to report metrics of the system. For instance, total job time, statistics about the utilization of compute resources (including divergence statistics), memory bandwidth achieved, accuracy obtained, etc. We will also compare the performance achieved by using different kernels which can potentially help the user to tune the choice of using kernels. For instance, the Squared Exponential Kernel may work well on a certain type of workload, or the Rational Quadratic Kernel may work well on certain other types of workloads etc.</li>
<li>Compare the system to an efficient serial version that runs on a CPU. Also, we understand that just calculating speedups when code is run on CPU vs GPU is not the correct way of judging how well a system has been designed. Therefore, as discussed previously, we will do a “throughput per dollar” comparison for both platforms and draw conclusions from the same.</li>
<li>Design and implement several test suites. These will be used to measure the performance of the code and to compare it to the baseline models.</li>
</ul>


<h3>
<a id="presentation" class="anchor" href="#presentation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Presentation</h3>

<p>We will show the performance of the system under several workloads, and we will compare it to a serial version implemented on a CPU. We will show several graphs for different test data sets.</p>

<h3>
<a id="platform" class="anchor" href="#platform" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Platform Choice</h3>
<p>
We will use the C++ programming language for this project along with the Nvidia CUDA library. We will develop our code on the latedays cluster, since the cluster has the fastest Nvidia GPUs (Nvidia TitanX GPU) on campus, and we would be focusing on obtaining maximum performance for the hardware on latedays.
</p>


<h3>
<a id="resources" class="anchor" href="#resources" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Resources</h3>

<p>We will start our code base from scratch.</p>



<h3>
<a id="schedule" class="anchor" href="#schedule" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Schedule</h3>

<ul>

<li>2nd April</li>

<ul>
<li>Install Magma, Lapack, Atlas, ACML, BLAS, CBLAS. - <b>DONE (but not really needed for us)</b></li>
</ul>

<li>8th April</li>

<ul>
<li>Get a sequential code for GP working. Implement at least 2 Covariance Kernels - <b>DONE</b></li>
<li>Test it on chosen workloads to get a fair estimate of how it performs - <b>DONE for sine dataset</b></li>
<li>Work on Design of the GPU version of the system, and discuss how to port the code to run on GPU - <b>DONE</b></li>
</ul>

<li>15th April (checkpoint)</li>

<ul>
<li>Implement one more covariance kernel - <b>PUSHED TO LOWER PRIORITY</b></li>
<li>Should have a naively parallelized code running on GPU - <b>DONE</b></li>
<li>Keep performing regression tests to ensure that the porting of code is correct - <b>DONE</b></li>
</ul>

<li>22nd April</li>

<ul>
<li>Reevaluate design of parallel code and optimize for performance, locality, access patterns - <b>IN PROGRESS</b> </li>
<li>Read research papers to extend our current design and possibly implement different strategies - <b>IN PROGRESS</b> </li>
<li>Start work on performance evaluation to find places where the framework takes a long time. Find bottlenecks - <b>IN PROGRESS</b> </li>
</ul>

<li>29th April</li>
<ul>
<li>Investigate scalability of the framework, both - with increasing number of cores, and with increasing data sets. Does the framework scale?</li>
<li>Investigate effects of different covariance kernels. Find reasons for why they perform as they do.</li>
</ul>

<li>4th May</li>
<ul>
<li>Finish all work</li>
<li>Evaluate system performance and fine tune different parameters</li>
</ul>

<li>7th May</li>
<ul>
<li>Code drop in</li>
<li>Finish writeup</li>
</ul>
</ul>


-->
<h3>
<a id="Summary" class="anchor" href="#summary" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Summary</h3>

<p>We have parallelized the training and prediction phases of Gaussian Process Regression on a GPU. We also took it a step further and implemented a Distributed Approximate version: DAcuGP. This version effectively parallelizes all computation across several GPUs in a cluster while also managing light-weight communication between the participating nodes. We were able to obtain significant speedups against strong baselines. Furthermore, to be able to handle arbitrary sized data sets, we extended the approach by adding a decentralized computation-scheduler that appropriately dispatches work across nodes.</p>

<h3>
<a id="background" class="anchor" href="#background" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Background</h3>

<p>Gaussian Process (GP) regression models have become very popular in recent years in the machine learning community. This can be attributed to the flexibility offered by these models to allow users to choose the appropriate covariance function to control the degree of smoothness. GP regression is basically a Bayesian nonparametric model (where an underlying finite-dimensional random variable is replaced by a stochastic process). Formally, a GP is a stochastic process where any finite number of random variables have a joint Gaussian distribution [RW06]. </p>

<p>A GP is completely specified by a mean function ($m(\vec{x})$) and a covariance function ($\kappa (\vec{x}_1, \vec{x}_2)$), which for a real process $f(\vec{x})$ can be defined as:</p>

<p>$$ m(\vec{x}) = \mathbf{E}[f(\vec{x})] \ 
\kappa (\vec{x}_1, \vec{x}_2) =  \mathbf{E}[(f(\vec{x}_1) - m(\vec{x}_1))( f(\vec{x}_2) - m(\vec{x}_2)  )] $$</p>

<p>Generally, the mean function is taken as zero. In that case, the covariance function  ($\kappa (\vec{x}_1, \vec{x}_2)$) basically turns into the covariance between $f(\vec{x}_1)$ and $f(\vec{x}_2)$. The squared exponential (SE) kernel is a common choice of covariance function (amongst a wide range of available choices for the kernel), and is defined as follows:
$$ \kappa (\vec{x}_1, \vec{x}_2)  = \exp( -\frac{1}{2l^2} | \vec{x}_1 -  \vec{x}_2 |^2 ),$$
where, $l$ is the lengthscale parameter that needs to be estimated.</p>

<p>In order to perform nonparametric regression, a GP prior is placed over the unknown function. The posterior distribution obtained by multiplying the prior with the likelihood is again a GP. The predictive distribution for a new test point ( $ \vec{x}_{t} $ ), given the entire dataset ($n$ tuples of the form $(\vec{x}, y)$) can be shown to be a Gaussian with the following mean ( $ \hat{f_{t}} $ ) and variance ( $ \mathbf{V}[f_{t}] $ ):
$$ \hat{f_{t}} = {\vec{k}_{t}}^{T} (K + \sigma_{n}^2I)^{-1} \vec{y} $$

$$ \mathbf{V}[f_{t}] = \kappa (\vec{x}_{t}, \vec{x}_{t}) - {\vec{k}_{t}}^{T} (K + \sigma_{n}^2I)^{-1} \vec{k}_{t} $$
where,  </p>

<ul>
<li> $K$ is a $n \times n$ matrix where the $ij$th entry is computed as $K_{ij} = \kappa (\vec{x}_i, \vec{x}_j)$,
<li>$\vec{y}$ is a $n \times 1$ vector formed by stacking the $n$ target values together from the dataset,</li>
<li>$I$ is a $n \times n$ identity matrix,</li>
<li>$\sigma^2$ is the noise variance,</li>
<li>$ \vec{k}_{t}$ is a $n \times 1$ vector where each entry is computed as $ \vec{k}_{t_{i}} = \kappa (\vec{x}_i, \vec{x}_{t})$</li>
</ul>


<p>In order to learn the different parameters (lengthscale(s) of the covariance kernel, the noise and signal variances), a common technique is to use the marginal likelihood maximization framework (this step can be thought of as the 'training' phase). The log marginal likelihood has the following form:</p>

<p>$$ LML = \log p(\vec{y} | X) = -\frac{1}{2} \vec{y}^T (K + \sigma_{n}^2I)^{-1}\vec{y} - \frac{1}{2} \log |K + \sigma_{n}^2I | - \frac{n}{2} \log 2\pi    $$  </p>

<p>If we consider $\vec{\theta}$ as the vector of parameters to estimate, gradient based methods (conjugate gradient of L-BFGS) can be employed after obtaining suitable analytic gradients $ \frac{\partial LML}{ \partial \vec{\theta}_i}$.</p>

<p>The main bottlenecks for both training and prediction phases are the compute- and memory-intensive matrix computations, which take $\mathcal{O}(n^3)$ flops, thus limiting the application of GPs for large datasets having $ n &gt; 10000$ samples.</p>

<p>Our main task in this project was to exploit the data parallelism available while handling all matrix computations. If time permits we would like to look at recent approximate approaches that might be more suitable for distributed GPU setting. [DN15, WN15]</p>



<h3>
<a id="approach" class="anchor" href="#approach" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Approach</h3>

<ul>

<li> <u> Serial implementation </u>

<p>In order to gain a better understanding of each of the operations involved in both learning (training) and inference (testing) phases, we implemented a serial single-threaded version of the GP regression algorithm. To accomplish this, we implemented our own mini matrix algebra library for all the required operations in GPR. This mini library has routines for Cholesky decomposition, forward substitution, backward substitution among other standard matrix-matrix/matrix-vector operations.  For minimizing the NLML in the training phase, we employed the conjugate gradient solver by Joao Cunha.  To be precise, this is the only third party code that we used in our serial implementation, all other routines were written from scratch in C++. Correctness was established by cross-checking outputs of each of the steps with the corresponding results as in the popular GPML framework which is written in Matlab. </p>

</li>

<li> <u> GPR on CUDA: First attempt </u>
<p> After acquainting ourselves with the nitty gritty details at each step, we implemented GP regression on CUDA writing our own kernels for each task. As in the previous case, we implemented the kernels for the required matrix computations ourselves. More precisely, we wrote a recursive blocked version of Cholesky decomposition as described in the paper by Dong et al [DH14]. One of the important steps in computing the likelihood as well as in computing the gradients,  is solving a triangular system of equations. An efficient method for solving such a system is to employ forward/backward substitution algorithm. Since both the algorithms are inherently sequential, we implemented a recursive algorithm for efficiently finding the inverse of a lower(upper) triangular matrix as given in []. We also implemented a reasonably fast matrix-matrix multiplication kernel that uses shared memory. Moreover, implementing the kernels ourselves gave us the opportunity to fuse different operations together. We benchmark our implementation on a synthetic sine dataset, wherein the input data lives in a 10 dimensional space, and the function to approximate is a sine function that depends only on the first dimension. In order to make the targets (function obsrevations) noisy, we add a Gaussian noise to the function values (the variance of this Gaussian is 0.05). Figure 1 shows the performance of the most crucial functions (some of which are implemented with a series of kernels), on the synthetic dataset. Note that all our experiments were performed using the K40 GPUs. </p>

<img src="images/1_naiveparallelmicrobench.png" alt="naiveparallelmicrobench" height="500" width="600" align="middle">

<p>We see that computation of the gradient of log hyperparameters takes the most amout of time. This is expected because the gradient compuatoin involves an inverse of  a lower traingular matrix and generation of the covariance matrix, in addition to vector-vector dot products and matrix-vector dot products and vector-vector dot product. In addition to providing significant insights, this implementation serves as strong baseline for our actual approach.  </p>
</li>

<li> <u> cuGP </u>

<p>For  our final approach, we build on the version-1 of our CUDA implemenation. We replaced some of our kernels with routines from cuBLAS/cuSOLVER. More specifically, we employ the following routines from the cuBLAS library:</p>
<p>DGEMM for efficient matrix multiplication</p>
<p>DGEMV for matrix-vector product</p>
<p>DGER: for vector outer product</p>
<p>DTRSM: for solving the triangular system of equations.</p>


<p>For fast Cholesky decomposition, we employ cuSOLVER’s DPTORF routine. Additionally, we use THRUST library routines for vector dot products and parallel reduce. However, we retain our important kernels like the one reponisibel for computing the covariance matrix, another which performs efficient elementwise matrix muliplication, and so on. </p>

<p>Furthermore, we optimize our other kernels by ensuring low bank contention on memory access conflicts by reading from addresses dictated by the logical location of a thread in the CUDA grid.</p>

<p>The performance of the main components in our final single node GP implementation in CUDA of the training phase (referred to as cuGP) is given in figure 2. The results are obtained on the previously described sine dataset.</p>

<img src="images/2_optimizedmicrobench.png" alt="optimizemicrobench" height="400" width="500" align="middle">

<table>
  <tr>
    <th></th>
    <th>Log Likelihood</th>
    <th>Gradient of hyper params</th>
    <th>Cholesky</th>
    <th>K inverse</th>
    <th>K train</th>
  </tr>
  <tr>
    <td><strong>1024</strong></td>
    <td>4.63</td>
    <td>23.95</td>
    <td>16.4</td>
    <td>192</td>
    <td>1.7</td>
  </tr>
  <tr>
    <td><strong>2048</strong></td>
    <td>5.91</td>
    <td>23.22</td>
    <td>26.5</td>
    <td>253</td>
    <td>1.77</td>
  </tr>
  <tr>
    <td><strong>4096</strong></td>
    <td>33.4</td>
    <td>28</td>
    <td>84.8</td>
    <td>511</td>
    <td>1.42</td>
  </tr>
</table>

</li>


<p>Considering our earlier approach as a competitive baseline, the speedup obtained with our cuGP implementation for the traing phrase is gvien in table.</p>

<p>Figure 3 compares the performance of cuGP with the baseline in the testing phase. The x-axis shows only the number of training points, the number of traning points for each experiment was 1000. We see signifiant speedup values over the baseline for both training as well as testing phaes.</p>

<img src="images/3_singlenodespeedup.png" alt="singlenode" height="400" width="500" align="middle">

<img src="images/7_parallelprediction.png" alt="parallelprediction" height="400" width="500" align="middle">

</li>


<li> <u> DA-cuGP: Distributed Approximate cuGP </u>

<p>The single node cuGP implementation performs well on datasets of the size O(10^4). Howerver, due to the O(N2) storage requirement, the GP implementation is limited by the amount of memory available on the device. To further scale , we have implemented a recent approach which is an approximation to the exact GP regreession. This was our stretch goal up till the checkpoint. We adapt the method to mulitple GPUs. The high level idea is of the approximate approach is to have several experts, each of whom is responsible for a subset of the original data. The approach basically approximates the originial covariance matrix to several blocks.</p>

<p>Thus the negative log marginal likelihood can be approximated by the sum of the log marginal likelihood values of each individual expert. The subtle point in the learning (training) phase is that the hyperparameters have to be shared across the products.</p>

<p>This approach is similar to the parameter-server approach as discussed in the class. So one node (a master node) is responsible for storing the current hyperparamters of the GP model. At each step of the log likelihood / gradient compuaation, each of the workers (experts in our context) need to get the latest value of the hyperparameters, and perform the required computation. The master then collects the partial results and updates the hyperparameters and broadcasts back the new parameters. </p>

<p>So we have implemented this approach using mulitple GPUs (which can be thought of as individual experts). In order to communicate the hyper-parameters, we use socke programming for efficiently transferrring the information to and from the GPUs. We call our implementation Distributed Approximate cuGP (DAcuGP).</p>

<p>The performance of the DAcuGP on the synthetic sine dataset in shown in Figure 4. The first bar corresponds the exact GP working on a dataset with 10000 points. As we move towards the right, the number of GPUs increase and the size of the dataset per GPU decreases. As one might expect, we obtain massive speedup due to both reduction of the problem size, and asymptotically lower complexity of the approximate method itself.</p>


<img src="images/4_distributedspeedup.png" alt="distributed" height="400" width="500" align="middle">

</li>

<li> <u> DAS-cuGP: Distributed Approximate Scalable cuGP </u>

<p>The size of data that can be handled by DA-cuGP is limited by the number of GPUs available. In order to further scale up,  we wrote a decentralized scheduler which can handle arbitary sizes of data. The basic idea is the same, we shard the data into smaller subsets. And, we map each of the shards to the avalable GPUs in as similar way as is done in an ISPC gang. </p>

<p>To provide a high level intuition, let’s suppose that we have 8 shards and only 2 GPUs are available. FOr computitgh the log likelihood at a single step requires input from all the experts (from all the shards), so the first GPU is assigned to compute the ll for shards 1 3 5 7, and the second GPU computes ll for the remaninig shards in an iteratiove fasthion (it is imortant to note that only 1 shard will fit in a GPU at one point of time).</p>

<p>So essentially the data locality is not there at all. But the tradeoff is that we can manage greater volumes of data. </p>

<p>To hide the latency of reading a shard each time, we implemented a prefetcher, which is responsible for bringing the next chunk of data to the memory in host, at the time when the device is performing necessary computaion with the shard in the device memory. We call this the Distributed Approximate Scalable cuGP (DAScuGP) approach.</p>


<p>Figure 7 shows the improvement performance due to the preferethch on a dataset comprising of 24000 points which are divided inoto 16 shards, when employihng 4 GPUs. </p>
<img src="images/6_latencyhiding.png" alt="latencyhiding" height="400" width="500" align="middle">


<p>In order to demonstrate that DAScuGP enables us to handle large amounts of data, we applied our method to the US Flight delay dataset. Given the amount of time for running a job in latedays cluster (3 hours), we subsampled the original data to obtain 1 million samples. There are 7 attributes which are used to predict the flight arrival delay time. We divide this dataset into 200 shards each having 5000 samples. Figure 8 shows the time required for DAS-cuGP  while varying the number of shards. We find that it takes less than 3 hours to do the learning phase in DAS-cuGP.</p>

<img src="images/5_scaling.png" alt="scaling" height="400" width="500" align="middle">
</li>

</ul>


<h3>
<a id="references" class="anchor" href="#references" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>References</h3>

<p>[RW06]: Williams, Christopher KI, and Carl Edward Rasmussen. "Gaussian processes for machine learning." the MIT Press 2.3 (2006): 4.</p>

<p>[WN15]: Wilson, Andrew Gordon, and Hannes Nickisch. "Kernel interpolation for scalable structured Gaussian processes (KISS-GP)." arXiv preprint arXiv:1503.01057 (2015).</p>

<p>[DN15]: Deisenroth, Marc Peter, and Jun Wei Ng. "Distributed Gaussian Processes." arXiv preprint arXiv:1502.02843 (2015).</p>

<p>[DA14]: Dong, Tingxing, et al. "A fast batched Cholesky factorization on a GPU." Parallel Processing (ICPP), 2014 43rd International Conference on. IEEE (2014)</p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/abhishekjoshi2/cuGP">cuGP</a> is maintained by <a href="https://github.com/sidgoyal78">sidgoyal78</a> and <a href="https://github.com/abhishekjoshi2">abhishekjoshi2</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>

